%% Prace z cyklu

@article{C1,
title = {Statistical Drift Detection Ensemble for batch processing of data streams},
journal = {Knowledge-Based Systems},
volume = {252},
pages = {109380},
year = {2022},
%issn = {0950-7051},
doi = {10.1016/j.knosys.2022.109380},
%url = {https://www.sciencedirect.com/science/article/pii/S095070512200692X},
author = {Joanna Komorniczak and Paweł Zyblewski and Paweł Ksieniewicz},
keywords = {Data streams, Concept drift, Drift detection, Statistical drift detection, Classification}
}
	
@article{C2,
  doi = {10.1007/s10489-022-03826-4},
%  url = {10.1007/s10489-022-03826-4},
  year = {2022},
  month = jul,
  publisher = {Springer Science and Business Media {LLC}},
  author = {Pawel Ksieniewicz},
  title = {Processing data stream with chunk-similarity model selection},
  journal = {Applied Intelligence}
}

@inproceedings{C3,
	author = {Joanna Komorniczak and Pawel Zyblewski and Pawel Ksieniewicz},
	booktitle = {2021 International Joint Conference on Neural Networks ({IJCNN})},
	date-added = {2021-11-17 13:45:07 +0100},
	date-modified = {2021-11-17 13:45:07 +0100},
	doi = {10.1109/ijcnn52387.2021.9533795},
	month = jul,
	publisher = {{IEEE}},
	title = {Prior Probability Estimation in Dynamically Imbalanced Data Streams},
	%url = {https://doi.org/10.1109/ijcnn52387.2021.9533795},
	year = {2021},
	%Bdsk-Url-1 = {https://doi.org/10.1109/ijcnn52387.2021.9533795}
}

@article{C4,
	author = {Pawe{\l} Ksieniewicz},
	date-added = {2021-11-17 13:43:41 +0100},
	date-modified = {2021-11-17 14:14:49 +0100},
	doi = {10.1016/j.neucom.2019.11.126},
	journal = {Neurocomputing},
	month = sep,
	pages = {309--316},
	publisher = {Elsevier {BV}},
	title = {The prior probability in the batch classification of imbalanced data streams},
	volume = {452},
	year = {2021},
}
	
@INPROCEEDINGS{C5,  
	author={Ksieniewicz, Paweł and Zyblewski, Paweł and Choraś, Michał and Kozik, Rafał and Giełczyk, Agata and Woźniak, Michał},  
	booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},   
	title={Fake News Detection from Data Streams},   
	year={2020},  
	volume={},  
	number={},  
	pages={1-8},  
	doi={10.1109/IJCNN48605.2020.9207498}
}
	
@InProceedings{C6,
author="Ksieniewicz, Pawel",
editor="P{\'e}rez Garc{\'i}a, Hilde
and S{\'a}nchez Gonz{\'a}lez, Lidia
and Castej{\'o}n Limas, Manuel
and Quinti{\'a}n Pardo, H{\'e}ctor
and Corchado Rodr{\'i}guez, Emilio",
title="Combining Random Subspace Approach with smote Oversampling for Imbalanced Data Classification",
booktitle="Hybrid Artificial Intelligent Systems",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="660--673",
abstract="Following work tries to utilize a hybrid approach of combining Random Subspace method and smote oversampling to solve a problem of imbalanced data classification. Paper contains a proposition of the ensemble diversified using Random Subspace approach, trained with a set oversampled in the context of each reduced subset of features. Algorithm was evaluated on the basis of the computer experiments carried out on the benchmark datasets and three different base classifiers.",
isbn="978-3-030-29859-3",
doi="10.1007/978-3-030-29859-3_56"
}

@article{C7,
title = {Data stream classification using active learned neural networks},
journal = {Neurocomputing},
volume = {353},
pages = {74-82},
year = {2019},
doi = {10.1016/j.neucom.2018.05.130},
author = {Paweł Ksieniewicz and Michał Woźniak and Bogusław Cyganek and Andrzej Kasprzak and Krzysztof Walkowiak},
keywords = {Pattern classification, Data stream, Active learning, Concept drift, Forgetting}
}

@InProceedings{C8,
  title = 	 {Undersampled Majority Class Ensemble for highly imbalanced binary classification},
  author =       {Ksieniewicz, Pawel},
  booktitle = 	 {Proceedings of the Second International Workshop on Learning with Imbalanced Domains: Theory and Applications},
  pages = 	 {82--94},
  year = 	 {2018},
  editor = 	 {Torgo, Luís and Matwin, Stan and Japkowicz, Nathalie and Krawczyk, Bartosz and Moniz, Nuno and Branco, Paula},
  volume = 	 {94},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10 Sep},
  publisher =    {PMLR},
  %pdf = 	 {http://proceedings.mlr.press/v94/ksieniewicz18a/ksieniewicz18a.pdf},
  url = 	 {https://proceedings.mlr.press/v94/ksieniewicz18a.html},
  abstract = 	 {Following work tries to utilize an ensemble approach to solve a problem of highly imbalanced data classification. Paper contains a proposition of umce – a multiple classifier system, based on k-fold division of the majority class to create a pool of classifiers breaking one imbalanced problem into many balanced ones while ensuring the presence of all available samples in the training procedure. Algorithm, with five proposed fusers and a pruning method based on the statistical dependencies of the classifiers response on the testing set, was evaluated on the basis of the computer experiments carried out on the benchmark datasets and two different base classifiers.}
}

@InProceedings{C9,
doi="10.1007/978-3-030-03496-2_33",
author="Ksieniewicz, Pawe{\l}
and Wo{\'{z}}niak, Micha{\l}",
editor="Yin, Hujun
and Camacho, David
and Novais, Paulo
and Tall{\'o}n-Ballesteros, Antonio J.",
title="Imbalanced Data Classification Based on Feature Selection Techniques",
booktitle="Intelligent Data Engineering and Automated Learning -- IDEAL 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="296--303",
abstract="The difficulty of the many classification tasks lies in the analyzed data nature, as disproportionate number of examples from different class in a learning set. Ignoring this characteristics causes that canonical classifiers display strongly biased performance on imbalanced datasets. In this work a novel classifier ensemble forming technique for imbalanced datasets is presented. On the one hand it takes into consideration selected features used for training individual classifiers, on the other hand it ensures an appropriate diversity of a classifier ensemble. The proposed method was tested on the basis of the computer experiments carried out on the several benchmark datasets. Their results seem to confirm the usefulness of the proposed concept.",
%isbn="978-3-030-03496-2"
}

@article{C10,
title = {Ensemble of Extreme Learning Machines with trained classifier combination and statistical features for hyperspectral data},
journal = {Neurocomputing},
volume = {271},
pages = {28-37},
year = {2018},
%issn = {0925-2312},
doi = {10.1016/j.neucom.2016.04.076},
%url = {https://www.sciencedirect.com/science/article/pii/S0925231217312195},
author = {Paweł Ksieniewicz and Bartosz Krawczyk and Michał Woźniak},
keywords = {Ensemble learning, Extreme Learning Machines, Hyperspectral imaging, Computer vision, Feature extraction, Dimensionality reduction, Image classification},
}

@InProceedings{C11,
  title = 	 {Dealing with the task of imbalanced, multidimensional data classification using ensembles of exposers},
  author = 	 {Ksieniewicz, Paweł and Woźniak, Michał},
  booktitle = 	 {Proceedings of the First International Workshop on Learning with Imbalanced Domains: Theory and Applications},
  pages = 	 {164--175},
  year = 	 {2017},
  editor = 	 {Luís Torgo, Paula Branco and Moniz, Nuno},
  volume = 	 {74},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {22 Sep},
  publisher =    {PMLR},
  %pdf = 	 {http://proceedings.mlr.press/v74/ksieniewicz17a/ksieniewicz17a.pdf},
  url = 	 {https://proceedings.mlr.press/v74/ksieniewicz17a.html},
  abstract = 	 {Recently, the problem of imbalanced data is the focus of intense research of machine learning community. Following work tries to utilize an approach of transforming the data space into another where classification task may become easier. Paper contains a proposition of a tool, based on a photographic metaphor to build a classifier ensemble, combined with a random subspace approach. Developed solution is insensitive to a sample size and robust to dimension increase, which allows a regularization of feature space, reducing the impact of biased classes. The proposed approach was evaluated on the basis of the computer experiments carried out on the benchmark and synthetic datasets.}
}
